# l2 regularization parameter
l2Wt = 0.05
# if isSparse is true then sparse setting will be used 
isSparse = false
# learning rate or lamda for Stochastic gradient descent
learningRate = 0.5
# learning rate = learning rate * learningRateIncreaseFactor
learningRateIncreaseFactor = 1.1
# learning rate = learning rate / learningRateDecreaseFactor
learningRateDecreaseFactor = 1.1
# maximum number of iterations to run SGD for
MAX_ITERS = 100
# held out set size as percentage of available data. For ex: 10% means held out set = 10%, train set = 90% 
heldOutSetSizePercentage = 0
# held out set size in terms of number of examples. For ex: 1000 means held out set = 1000 examples, train set = rest  
heldOutSetSizeValue = -1
# size of set as percentage of available data used to compute log loss after going through train data once 
learningRateSetSizePercentage = 100
# size of set in terms of number of examples used to compute log loss after going through train data once
learningRateSetSizeValue = -1
# number of gradient descent steps since the last update of learning rate 
learningRateUpdateFrequency = 5
 # number of gradient descent steps since the last update of held out log loss
hLossUpdateFrequency = 5
